{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess Bot using Deep Reinforcement Learning and Monte Carlo Tree Search\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Sungwook Min\n",
    "- Aatyanth Thimma-Udayakumar\n",
    "- Vu Le\n",
    "- Haoyan Wan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "In this project, we aim to develop a chess-playing model using Deep Reinforcement Learning (DRL) with an attention-based transformer architecture, a novel variant of RL that has proven effective in continuous decision-making tasks like chess. The goal is to train an agent capable of making optimal chess moves based on learned strategies through self play using Monte Carlo Tree Search (MCTS) by balancing exploration and exploitation, rather than pre-programmed heuristics. By combining a transformer architecture-based Policy Neural Network (PNN) with MCTS, the agent progressively refines its move selection and win prediction accuracy through iterative self-play. Our model's performance is evaluated by comparing the ELO rating of our model against established chess engines like Stockfish and Leela Chess Zero, focussing on both raw performance and overall chess skill. [ADD THE RESULTS SUMMARY HERE; 1-2 SENTENCES ON ACCURACY, COMPARISON WITH OTHER CHESS ENGINES, AND WIN-LOSS RATION]. Ultimately, our work provides insights into the feasibility of applying reinforcement learning to chess engines and lays the groundwork for future optimization.\n",
    "\n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "[<sup>[1]</sup>]()\n",
    "The paper \"Acquisition of Chess Knowledge in AlphaZero\" explores how AlphaZero, a self-learning chess engine, learns through self-play, without any human guidance [<sup>[1]</sup>](). Unlike Stockfish, which relies on predefined heuristics and brute-force search, AlphaZero combines Monte Carlo Tree Search (MCTS) with a deep neural network to evaluate positions dynamically [<sup>[1]</sup>](). It develops concepts such as piece values, mobility, king safety, and material advantage, which emerge naturally in its decision-making process. Initially playing random moves, AlphaZero refines its strategies between 25k to 60k training steps, finding optimal openings and improving its endgame play [<sup>[1]</sup>](). AlphaZero optimizes moves purely based on effectiveness, resulting in a highly efficient playing style. However, it's performance deteriorates on lower-end gpus. The study highlights how AlphaZero surpasses traditional engines like Stockfish by generalizing chess knowledge rather than relying on static rules, raising broader implications for AI learning and decision-making beyond chess. \n",
    "______________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "[<sup>[2]</sup>]()Furthermore, the paper, \"Chess Moves Prediction using Deep Learning Neural Networks\", examines the use of CNNs to predict chess moves, training on 1.5 million board states. While the model achieved 39.16% accuracy, it struggled with long-term planning and lost 95% of games to Stockfish, indicating that CNNs alone are insufficient for strong chess AI [<sup>[2]</sup>](). Unlike AlphaZero, which integrates CNNs with Monte Carlo Tree Search (MCTS) and reinforcement learning, this model relied on supervised learning, limiting its adaptability and generalization beyond its training data [<sup>[2]</sup>](). The study highlights that while CNNs can recognize positional and tactical patterns, they lack the deep search capability needed for high-level play. The model showed a tendency to prioritize piece activity over long-term positional advantages, sometimes sacrificing material without fully evaluating the consequences [<sup>[2]</sup>](). With the absence of a reinforcement learning component meant it could not self-improve through gameplay. These findings suggest that PNN + MCTS could be a viable alternative, potentially offering a more computationally efficient yet effective approach to chess AI by combining PNNs' lightweight evaluation with MCTS' deep search capabilities.\n",
    "______________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "[<sup>[3]</sup>]() The paper \"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\\\" presents AlphaZero, a reinforcement learning system that masters chess, shogi, and Go through self-play without human-designed heuristics [<sup>[3]</sup>](). In a 100-game match against Stockfish, AlphaZero won 25 games as White, 3 games as Black, and drew the remaining 72 games—losing none [<sup>[3]</sup>](). Despite searching only 80,000 positions per second compared to Stockfish’s 70 million, AlphaZero’s learning-based evaluation and selective search enabled it to dominate [<sup>[3]</sup>](). This demonstrates the efficiency of deep reinforcement learning in strategic decision-making, surpassing traditional rule-based approaches.,\n",
    "______________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "[<sup>[4]</sup>]() The paper \"Mastering Chess with a Transformer Model\" introduces Chessformer, a transformer-based chess engine designed to outperform AlphaZero while using significantly fewer computational resources [<sup>[4]</sup>](). Unlike traditional engines like Stockfish, which rely on brute-force search and handcrafted evaluation functions, or reinforcement learning-based engines like AlphaZero, which combine deep neural networks with Monte Carlo Tree Search (MCTS), Chessformer leverages transformer-based self-attention mechanisms to optimize move selection [<sup>[4]</sup>](). The study demonstrates that Chessformer achieves grandmaster-level performance, matching or surpassing AlphaZero’s playing strength and puzzle-solving ability while requiring 8× to 30× less computation [<sup>[4]</sup>](). Unlike convolutional architectures, which struggle with long-range dependencies in chess, Chessformer effectively models positional relationships, allowing it to recognize complex strategic motifs such as trapped pieces—patterns that often elude traditional search-based engines. This suggests that transformer-based architectures, with their ability to generalize efficiently from self-play.\n",
    "\n",
    "______________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "Reinforcement learning-based chess engines trained solely through self-play, like AlphaZero, have shown superior efficiency compared to traditional search-based engines like Stockfish and hybrid models like Leela Chess Zero. AlphaZero, using MCTS and deep neural networks, outperformed Stockfish while analyzing significantly fewer positions. However, newer transformer-based models like Chessformer improve on this by leveraging self-attention to enhance positional understanding with far less computation. Given these advancements, a transformer-based Policy Neural Network (PNN) could further refine chess AI efficiency. This project will implement and evaluate such a model, using the Elo rating system to measure its strategic effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "***How does a reinforcement learning-based chess engine compare to traditional search-based engines like Stockfish and hybrid engines like Leela Chess Zero in terms of raw performance (ELO Rating) and decision-making efficiency? While traditional engines rely on brute-force and heuristic based approaches, a reinforcement learning approach will help the model learn the most optimal move to take at each state through exploration and exploitation of the chess environment rather than pre-defined rules. To investigate this experience based approach, we will develop a deep reinforcement learning chess engine using an attention-based transformer architecture and Monte Carlo tree search. Our model will then be evaluated using an ELO rating system on our main metrics:*** \n",
    "\n",
    "    - Raw Performance\n",
    "    - Overall Skill Level\n",
    "    \n",
    "***By iteratively tuning our model and benchmarking it against well established chess engines, we aim to asses whether Reinforcement Learning can yield a competitive AI chess engine.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "***For this project, we will not be using any external data sets as we seek to train the model on Reinforcement Learning (RL).***\n",
    "\n",
    "- ***State Space Representation:***\n",
    "\n",
    "    Chess board representation:\n",
    "    - White Pieces:\n",
    "        - Pawns (P): 1\n",
    "        - Knights (N) : 2\n",
    "        - Bishops (B) : 3\n",
    "        - Rooks (R) : 4\n",
    "        - Queens (Q) : 5\n",
    "        - Kings (K) : 6\n",
    "    - Black pieces:\n",
    "        - Pawns (p) → 7\n",
    "        - Knights (n) → 8\n",
    "        - Bishops (b) → 9\n",
    "        - Rooks (r) → 10\n",
    "        - Queens (q) → 11\n",
    "        - Kings (k) → 12\n",
    "        - Empty squares → 0\n",
    "\n",
    "    Each board position is represented in a (1,64) tensor, with each element representing an piece with it's encoded value.\n",
    "    Player turn is also represented as a tensor, with 1 for White and 0 for Black.\n",
    "\n",
    "    - Castling:\n",
    "        - Each player's castling is tracked seperatley, w_kingside, w_queenside, b_kingside, b_queenside, with a value of 1 if allowed and 0 if it isn't.\n",
    "\n",
    "    - Move Representation:\n",
    "        - Moves are indexed by UCI (Universal Chess Interface) encoding, which represents moves in algebraic notation (ex: e2e4)\n",
    "        - Legal moves are filtered through a mask\n",
    "    \n",
    "    - Position History:\n",
    "        - In order to prevent repeated positions, we penalize repeated moves stored in a dictionary to avoid loops.\n",
    "\n",
    "    - Transformer Model Input Representation:\n",
    "        - The inputs to the model consists of board_positions, turns, and castling rights.\n",
    "        - batch = {\n",
    "            \"board_positions\": board_tensor.to(device),\n",
    "            \"turns\": turn_tensor.to(device),\n",
    "            \"white_kingside_castling_rights\": w_kingside.to(device),\n",
    "            \"white_queenside_castling_rights\": w_queenside.to(device),\n",
    "            \"black_kingside_castling_rights\": b_kingside.to(device),\n",
    "            \"black_queenside_castling_rights\": b_queenside.to(device)\n",
    "            }\n",
    "    \n",
    "    - Policy Neural Network Output Structure\n",
    "        - Policy Output (Move): A probability distribution over all UCI-encoded moves\n",
    "        - Value output: A singular scalar inside the range [-1,1] which represents the model's evaluation of the position\n",
    "        - The policy head is trained cross-entropy loss\n",
    "        - The value head is trained using Mean-Squared-Error\n",
    "    \n",
    "\n",
    "    - FEEDBACK TO ADDRESS FROM SCOTT:\n",
    "        The proposal states the use of an attention-based transformer for the Policy NN, but it does not detail how the board state will be represented or how the transformer layers will be configured. What embedding strategy will you use for the chess board? For instance, how will you encode piece types, positions, and contextual information?\n",
    "\n",
    "\n",
    "Detail how/where you obtained the data and cleaned it (if necessary)\n",
    "\n",
    "If the data cleaning process is very long (e.g., elaborate text processing) consider describing it briefly here in text, and moving the actual clearning process to another notebook in your repo (include a link here!).  The idea behind this approach: this is a report, and if you blow up the flow of the report to include a lot of code it makes it hard to read.\n",
    "\n",
    "Please give the following infomration for each dataset you are using\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "***We propose a solution similar to that of AlphaZero using Deep Reinforcement Learning. Our approach involves creating a Policy Neural Network (Policy NN) that takes in the board state as input and produces two outputs: (move_distribution, win_percentage). The unique aspect of our implementation is that the Policy NN will be built using an attention-based transformer architecture instead of traditional convolutional networks. The output head will consist of an 8×8×73 tensor, representing all possible moves in chess.***\n",
    "\n",
    "***To train this network, we will set up a reinforcement learning environment using OpenAI Gym. We will create self-play agents that use the probabilities generated by the Policy NN, combined with Monte Carlo Tree Search (MCTS), to play games against themselves. At each step of the game, the agent will record the current board state, the move distribution obtained from MCTS, and the final win/loss outcome. The MCTS component ensures that move selection is biased for less explored branches, striking a balance between exploration and exploitation through mechanisms such as the PUCT (Predictor + Upper Confidence Bound for Trees) formula.***\n",
    "\n",
    "***After each self-play game, we will compute the loss function, which consists of three key terms:***\n",
    "\n",
    "***Mean Squared Error (MSE) loss for win percentage prediction, ensuring that the value function correctly estimates the probability of winning from a given board state.***\n",
    "***Cross-Entropy Loss between the MCTS move distribution and the policy network’s move distribution, refining the policy network to match the improved move selection of MCTS.***\n",
    "***A regularization term to prevent overfitting and stabilize training.***\n",
    "***The loss will be backpropagated through gradient descent, updating the Policy NN weights. This process will repeat continuously, with the updated policy network generating improved move distributions, leading to stronger self-play agents over time.***\n",
    "\n",
    "***This iterative cycle of self-play, MCTS-guided move selection, and policy refinement ensures that the model improves progressively, discovering stronger moves while maintaining a balance between exploration (searching for new strategies) and exploitation (using known strong moves effectively).***\n",
    "\n",
    "- In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.  \n",
    "\n",
    "- If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "- If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "***Elo Rating System (Skill Level)***\n",
    "\n",
    "- ***Mathematical Formulation***\n",
    "    - ***Final ELO Calculation: R' = R + K(S - E)***\n",
    "        - *R': New rating*\n",
    "        - *R: Old rating*\n",
    "        - *K: Constant to dictate how much to change the rating (analogous to the learning rate ${\\alpha}$ in MC, DP, and TD)*\n",
    "            - *Usually around 30*\n",
    "        - *S: Actual score (1 for win, 0.5 for draw, or 0 for loss)*\n",
    "        - *E: Expected score*\n",
    "    - ***Expected Score Calculation:*** $E = \\frac{1}{1 + {10}^{\\frac{R_{opponent} - R}{400}}}$\n",
    "        - *$R_{opponent}$*: *Opponent's rating*\n",
    "        - *$R$: Player's rating*\n",
    "\n",
    "- ***Explanation***\n",
    "    - *The first equation represents the rule used to update the ELO rating of the model/player/bot after playing a game*\n",
    "        - *This equation will penalize the model significantly more if it loses to a player/bot with a lower ELO rating than if it was to lose to a player/bot with a higher ELO rating*\n",
    "        - *Subsequently, if the model beats a player/bot with a higher ELO rating, the new ELO rating of the bot will increase significantly more than if it were to beat a player/bot with a lower ELO rating*\n",
    "    - *The second equation represents the probability that the model will win the game based on their current ELO rating compared to their opponents ELO rating*\n",
    "        - *This equation helps quantify the capability of the model with respect to its opponent*\n",
    "            - *For instance, if the opponent and the model have a similar ELO rating, then $E_{opponent}$ and $E$ will be 0.5 since $R_{opponent}$ = $R$*\n",
    "            - *Subsequently, if one player is significantly better than the other, then their expected score will be closer to 1 and if a player is significantly worse than the other, then their expected score will be closer to 0*\n",
    "    - *This probability value will help proportionally adjust the ELO ratings based on the outcome with respect to the difference in ELO (from the start of the game)*\n",
    "        - *i.e: penalize the model significantly more if it loses to a player/bot with a lower ELO rating*\n",
    "    - *In general, the higher the ELO rating, the better the model/bot/player is*\n",
    "- ***Relevance***\n",
    "    - *The ELO score represents how good of a chess player the model is*\n",
    "        - *Important to differentiate that just because the model is a good chess player doesn't necessarily translate to the model always picking the best possible move at each state*\n",
    "    - *ELO updates are done with respect to the model's current skill level and the opponent's skill level*\n",
    "        - *Proportionally rewards/penalizes model (i.e: penalize the model significantly more if it loses to a player/bot with a lower ELO rating)*\n",
    "        - *The final ELO score of our model, for our project purposes, is derived through repeated matches with known chess engines of various ratings (will be elaborated on more in further sections)*\n",
    "    - The ELO rating of our model can be directly compared against that of other models to evaluate how well our model performs\n",
    "        - i.e: The ELO rating is a normalized metric, allowing direct comparison across different models and real players\n",
    "\n",
    "***Win-Loss Ratio (Raw Performance)***\n",
    "- ***Mathematical Formulation***\n",
    "    - ***Win-Loss Ratio:*** $\\frac{Wins}{Losses}$\n",
    "- ***Explanations:***\n",
    "    - Win-Loss Ratio > 1 represents the model wins more often than it loses\n",
    "    - Win-Loss Ratio = 1 represents that the model wins just as often as it loses\n",
    "    - Win-Loss Ratio < 1 represents that the model loses more often than it wins\n",
    "- ***Relevance***\n",
    "    - *Contextualizes how often our model wins vs loses*\n",
    "        - *Doesn't capture information regarding difficulty of opponents played*\n",
    "        - *Doesn't capture information regarding the efficiency of the moves chosen by our model*\n",
    "\n",
    "***Win Percentage (Raw Performance)***\n",
    "- ***Mathematical Formulation***\n",
    "    - ***Win Percentage:*** $\\frac{Wins}{Total Games} * 100$\n",
    "- ***Explanations:***\n",
    "    - *Percentage of games won by our model*\n",
    "    - *The higher the percentage, the more games the model wins*\n",
    "    - $0 \\leq$ ***Win Percentage*** $\\leq 100$\n",
    "- ***Relevance***\n",
    "    - *Contextualizes the proportion of games the model wins as a percentage*\n",
    "        - *Doesn't capture information regarding difficulty of opponents played*\n",
    "        - *Doesn't capture information regarding the efficiency of the moves chosen by our model*\n",
    "\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Probably you should include a learning curve to demonstrate how much better the model gets as you increase the number of trials\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Generally reinforement learning tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible.  Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?  Or you compare a completely different approach/alogirhtm to the problem? Whatever, this stuff is just serving suggestions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "- ***ELO Evaluation***\n",
    "  - *While the ELO rating gives us a good idea of how good of a chess player the model is, it doesn't necessarily indicate that the model understands positional play, endgames, long-term strategies, or effective decision making at each state*\n",
    "    - *This will be discussed in more detail next*\n",
    "  - *The ELO rating is more biased with fewer games played*\n",
    "    - *That is, the more games the model plays, the ELO rating will better demonstrate the actual skill level of the model*\n",
    "    - *This is because the model will start of with a base ELO (just like on chess.com) and that rating will adjust as the model plays more games*\n",
    "  - *For our model, we used a higher update constant (K) since we had limited time to evaluate the model*\n",
    "    - *This could have inflated or deflated the ELO rating of the model since the magnitude of changes being made to the ELO rating are bigger on average*\n",
    "    - *This tradeoff is analogous to using a larger vs smaller learning rate in a Reinforcement Learning algorithm (DP, MC, TD) or neural networks (i.e faster convergence with risk of overshooting vs slower convergence with risk of getting stuck in local minimas)*\n",
    "  - A solution to addressing these drawbacks would have been to let the model play against more opponents (preferably thousands) to generalize the ELO rating (skill level of the model)\n",
    "    - This would have also let us experiment with the optimal values for K to choose the best \"learning rate\"\n",
    "- ***Move Selection Evaluation***\n",
    "  - *As mentioned earlier, the ELO rating tells us how skilled the model is but not how efficient the model's move selection is for a given state*\n",
    "  - *While a higher ELO rating does indicate that the model is \"learning\" to make the best moves, it is not guaranteed that this improvement is based on the model actually \"learning\" what the best move would be*\n",
    "    - *i.e the model can just be memorizing patterns based on frequently visited states during self-play training or the model could be exploiting opponent weaknesses instead of learning the optimal moves*\n",
    "  - *Given the state-space of the chess environment, it was extremely difficult to evaluate how effective the model's move selection was*\n",
    "    - *Especially if we have to weigh the choice of the current move against all the other possible moves it could have taken as well as how this move affects the future state-spaces*\n",
    "    - *While training the model and observing the reward function helps us show that the model is learning to make better moves, we still didn't know exactly whether the model was \"learning\" to make better moves or was picking \"better\" moves based on some other factor*\n",
    "  - *Implementing a comprehensive move analysis metric would help us learn about the learning patterns of the model*\n",
    "    - *Based on this evaluation metric, we can directly penalize/reward the model or tune our hyperparameters to encourage the model towards learning the best moves for a state rather than other patterns*\n",
    "\n",
    "- ***Training Duration***\n",
    "  - *Monte Carlo based RL methods rely on the model interacting with the environment by striking a balance between exploration and exploitation which is both computational and temporally expensive*\n",
    "    - *Combined with our transformer based approach, our limited training time greatly restricted the model's knowledge of the state space(potentially causing the model to exploit or explore more over the other), decision making, and overall performance*\n",
    "    - *Having more time to train the model would have balanced the exploitation vs exploration dilemma allowing the model to explore more relevant states, increasing the model's knowledge of the state-space and improving it's move selection strategy*\n",
    "  - *The limited training time also decreased our ability to tune hyperparameters (learning rates,  discount factor, etc) as training the model each time is extremely computationally expensive and long*\n",
    "    - *Longer time to train our model would have allowed us to experiment with different hyperparameters  to optimize the model further*\n",
    "\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "\n",
    "### Future work\n",
    "- ***More Evaluation Metrics:***\n",
    "  - *One of the most important changes we can make in the future to improve our model's performance*\n",
    "  - *While implementing comprehensive evaluation metrics that evaluates the effectiveness of each move, blunder rate analysis, positional and end-game analysis, etc would be quite difficult, they will help us gather significantly more information regarding the models behavior*\n",
    "    - *This knowledge can be use to tweak hyperparameters or restructure our model to achieve our goal outlined in the problem statement*\n",
    "    - *With updated evaluation metrics, we can also conduct thorough experimentation to study what the model is doing and why to evaluate if the model's behavior mirrors expected behavior*\n",
    "- ***Optimizing training efficiency with Pre-training:***\n",
    "  - *Provides the model with basic knowledge of chess*\n",
    "  - *Doesn't force to model to learn everything from scratch*\n",
    "    - *Helps reduce training times*\n",
    "    - *Increases overall model performance since model will build upon pre-existing knowledge*\n",
    "  - *Reduced training time will also allow us to experiment with different hyperparameters*\n",
    "    - *i.e: Explore different exploration vs exploitation rates to increase the model's environment knowledge and ensure optimal action is picked*\n",
    "- ***Diversify Training Opponents:***\n",
    "  - *Not only rely on self-training*\n",
    "  - *Use the chess engines we are using for comparison in training as well (while ensuring proper train-test splits)*\n",
    "  - *Introduces different play-styles, skill levels, and variation in the opponents the model sees*\n",
    "    - *Helps reduce the risk of the model memorizing moves based on common states it observes*\n",
    "\n",
    "Looking at the limitations and/or the toughest parts of the problem and/or the situations where the algorithm(s) did the worst... is there something you'd like to try to make these better.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "* **Informed Consent**\n",
    "  * We will make sure to inform the owner of the gym-chess environment and the owner of the chess play data about the use of their code and gain persmission to use their source code.\n",
    "* **Data Security**\n",
    "  * Since the owner of the gym-chess environment and chess play data may not want public display of their source code in a third-party's repository, we will make sure to keep the data secure and make our repository private, only granting access to appropriate individuals.\n",
    "* **Data Storage**\n",
    "  * We plan to delete any data that we have collected for this project after the conclusion of the project.\n",
    "* **Data Representation**\n",
    "  * While cleaning and ensuring the data can be used for our project, we will make sure not to implement any new policies, inputs, or bias to represent the data in the best way possible.\n",
    "* **Fair Play**\n",
    "  * We will not produce the chess bot with the intent to bypass any anti-cheating mechanisms and will work to the best of our abilities to ensure the chess bot is not used for any misuse.\n",
    "* **AI Bias**\n",
    "  * We will train the chess bot to have a dynamic style of play and ensure that it doesn't develop a preference for a certain style of play.\n",
    "* **Interpretability**\n",
    "  * We will make sure to use appropriate techniques and visualizations, if applicable, to explain our model's decision making process to the best of our ability.\n",
    "* **Auditability**\n",
    "  * We will make sure that all code, data, visualizations, and results produced through our project will be reasonable and reproducible.\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "*Our primary objective through this project was to create an RL based chess engine that uses a learning-based approach to make optimal decisions as opposed to the brute force approaches employed by well established chess engines like Leela Chess Zero and Stockfish. We initially hypothesized that this learning-based approach should teach the model to behave like a human player through the experiential learning process enforced by the Monte Carlo Tree Search algorithm. Through our experiments, we saw [INSERT INFORMATION REGARDING THE RESULTS & COMPARISONS WITH OTHER CHESS ENGINES; TRY TO SUMMARIZE THIS IN 1-2 SENTENCES]. While these results demonstrate the progress achieved in our limited time frame, challenges such as the training time for the model and difficulty of integrating additional evaluation metrics constrained our ability to further optimize our model. However, future work can address these limitations by focussing on increasing the evaluation metrics, utilizing pre-training to optimize training efficiency, and diversifying the training opponents. Ultimately, our project lays the foundations for exploring Reinforcement Learning-based chess engines and establish the potential in this avenue of chess engines. With sufficient computational resources and extended training periods, future work can further enhance the effectiveness and competitiveness of such RL-based chess engines.*\n",
    "\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"Thomas McGrath, Andrei Kapishnikov, Nenad Tomašev, Adam Pearce, Martin Wattenberg, Demis Hassabis, Been Kim, Ulrich Paquet, and Vladimir Kramnik\"></a>1.[^](#McGrath): McGrath, Thomas et. al., (2022) Acquisition of chess knowledge in AlphaZero. *Proceedings of the National Academy of Sciences (PNAS)*. https://www.pnas.org/doi/epub/10.1073/pnas.2206625119<br> \n",
    "<a name=\"Hitanshu Panchal, Siddhant Mishra, and Varsha Shrivastava\"></a>2.[^](#lorenz): Panchal, Hitanshu et. al., (2021) Chess Moves Prediction using Deep Learning Neural Networks. *International Conference on Advances in Computing and Communications (ICACC)*. https://ieeexplore.ieee.org/abstract/document/9708405<br> \n",
    "\n",
    "\n",
    "<a name=\"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabish\\\"></a>3.[^](#Silver) Silver, Hubert, et. al., (2017) Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm. *DeepMind*. https://arxiv.org/pdf/1712.01815 <br>\n",
    "\n",
    "<a name =\"Daniel Monroe, Philip A. Chalmers\"></a>4.[^](#Monroe) Monroe, Chalmers (2024) Mastering Chess with a Transformer Model. https://arxiv.org/pdf/2409.12272"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
