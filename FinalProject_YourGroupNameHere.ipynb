{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess Bot using Deep Reinforcement Learning and Monte Carlo Tree Search\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Sungwook Min\n",
    "- Aatyanth Thimma-Udayakumar\n",
    "- Vu Le\n",
    "- Haoyan Wan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "For our final project, we aim to develop a chess-playing AI using Deep Reinforcement Learning with an attention-based transformer architecture, a reinforcement learning (RL) algorithm commonly used for continuous decision-making in complex environments like chess. The goal is to train an agent capable of making optimal chess moves based on learned strategies through trial and error rather than pre-programmed heuristics. Our chess bot will interact with an environment based on gym-chess to improve through self-play and reinforcement learning. Monte Carlo Tree Search (MCTS) will be integrated into the training pipeline to improve decision-making and move selection by balancing exploration and exploitation. Success will be measured using multiple performance metrics, including decision efficiency in game situations, evaluation of how powerful a certain move was, and of course, overall win-loss ratio. The Elo rating system will serve as our primary evaluation metric, where our chess bot will play against known chess engines of varying Elo. Through model tuning and iterative training, we hope to develop a competitive chess bot that is capable of making intelligent, high-quality moves.\n",
    "\n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "[<sup>[1]</sup>]()\n",
    "The paper \"Acquisition of Chess Knowledge in AlphaZero\" explores how AlphaZero, a self-learning chess engine, learns through self-play, without any human guidance [<sup>[1]</sup>](). Unlike Stockfish, which relies on predefined heuristics and brute-force search, AlphaZero combines Monte Carlo Tree Search (MCTS) with a deep neural network to evaluate positions dynamically [<sup>[1]</sup>](). It develops concepts such as piece values, mobility, king safety, and material advantage, which emerge naturally in its decision-making process. Initially playing random moves, AlphaZero refines its strategies between 25k to 60k training steps, finding optimal openings and improving its endgame play [<sup>[1]</sup>](). AlphaZero optimizes moves purely based on effectiveness, resulting in a highly efficient playing style. However, it's performance deteriorates on lower-end gpus. The study highlights how AlphaZero surpasses traditional engines like Stockfish by generalizing chess knowledge rather than relying on static rules, raising broader implications for AI learning and decision-making beyond chess. \n",
    "______________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "[<sup>[2]</sup>]()Furthermore, the paper, \"Chess Moves Prediction using Deep Learning Neural Networks\", examines the use of CNNs to predict chess moves, training on 1.5 million board states. While the model achieved 39.16% accuracy, it struggled with long-term planning and lost 95% of games to Stockfish, indicating that CNNs alone are insufficient for strong chess AI [<sup>[2]</sup>](). Unlike AlphaZero, which integrates CNNs with Monte Carlo Tree Search (MCTS) and reinforcement learning, this model relied on supervised learning, limiting its adaptability and generalization beyond its training data [<sup>[2]</sup>](). The study highlights that while CNNs can recognize positional and tactical patterns, they lack the deep search capability needed for high-level play. The model showed a tendency to prioritize piece activity over long-term positional advantages, sometimes sacrificing material without fully evaluating the consequences [<sup>[2]</sup>](). With the absence of a reinforcement learning component meant it could not self-improve through gameplay. These findings suggest that PNN + MCTS could be a viable alternative, potentially offering a more computationally efficient yet effective approach to chess AI by combining PNNs' lightweight evaluation with MCTS' deep search capabilities.\n",
    "______________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "[<sup>[3]</sup>]() The paper \"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\\\" presents AlphaZero, a reinforcement learning system that masters chess, shogi, and Go through self-play without human-designed heuristics [<sup>[3]</sup>](). In a 100-game match against Stockfish, AlphaZero won 25 games as White, 3 games as Black, and drew the remaining 72 games—losing none [<sup>[3]</sup>](). Despite searching only 80,000 positions per second compared to Stockfish’s 70 million, AlphaZero’s learning-based evaluation and selective search enabled it to dominate [<sup>[3]</sup>](). This demonstrates the efficiency of deep reinforcement learning in strategic decision-making, surpassing traditional rule-based approaches.,\n",
    "______________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "[<sup>[4]</sup>]() The paper \"Mastering Chess with a Transformer Model\" introduces Chessformer, a transformer-based chess engine designed to outperform AlphaZero while using significantly fewer computational resources [<sup>[4]</sup>](). Unlike traditional engines like Stockfish, which rely on brute-force search and handcrafted evaluation functions, or reinforcement learning-based engines like AlphaZero, which combine deep neural networks with Monte Carlo Tree Search (MCTS), Chessformer leverages transformer-based self-attention mechanisms to optimize move selection [<sup>[4]</sup>](). The study demonstrates that Chessformer achieves grandmaster-level performance, matching or surpassing AlphaZero’s playing strength and puzzle-solving ability while requiring 8× to 30× less computation [<sup>[4]</sup>](). Unlike convolutional architectures, which struggle with long-range dependencies in chess, Chessformer effectively models positional relationships, allowing it to recognize complex strategic motifs such as trapped pieces—patterns that often elude traditional search-based engines. This suggests that transformer-based architectures, with their ability to generalize efficiently from self-play.\n",
    "\n",
    "______________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "Reinforcement learning-based chess engines trained solely through self-play, like AlphaZero, have shown superior efficiency compared to traditional search-based engines like Stockfish and hybrid models like Leela Chess Zero. AlphaZero, using MCTS and deep neural networks, outperformed Stockfish while analyzing significantly fewer positions. However, newer transformer-based models like Chessformer improve on this by leveraging self-attention to enhance positional understanding with far less computation. Given these advancements, a transformer-based Policy Neural Network (PNN) could further refine chess AI efficiency. This project will implement and evaluate such a model, using the Elo rating system to measure its strategic effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "***How does a reinforcement learning-based chess engine compare to traditional search-based engines like Stockfish and hybrid engines like Leela Chess Zero in terms of raw performance (ELO Rating) and decision-making efficiency? While traditional engines rely on brute-force and heuristic based approaches, a reinforcement learning approach will help the model learn the most optimal move to take at each state through exploration and exploitation of the chess environment rather than pre-defined rules. To investigate this experience based approach, we will develop a deep reinforcement learning chess engine using an attention-based transformer architecture and Monte Carlo tree search. Our model will then be evaluated on 2 main metrics:*** \n",
    "\n",
    "    - Raw Performance\n",
    "    - Decision Making Efficiency\n",
    "    \n",
    "***By iteratively tuning our model and benchmarking it against well established chess engines, we aim to asses whether Reinforcement Learning can yield a competitive AI chess engine.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "***For this project, we will not be using any external data sets as we seek to train the model on Reinforcement Learning (RL).***\n",
    "\n",
    "\n",
    "Detail how/where you obtained the data and cleaned it (if necessary)\n",
    "\n",
    "If the data cleaning process is very long (e.g., elaborate text processing) consider describing it briefly here in text, and moving the actual clearning process to another notebook in your repo (include a link here!).  The idea behind this approach: this is a report, and if you blow up the flow of the report to include a lot of code it makes it hard to read.\n",
    "\n",
    "Please give the following infomration for each dataset you are using\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "***We propose a solution similar to that of AlphaZero using Deep Reinforcement Learning. Our approach involves creating a Policy Neural Network (Policy NN) that takes in the board state as input and produces two outputs: (move_distribution, win_percentage). The unique aspect of our implementation is that the Policy NN will be built using an attention-based transformer architecture instead of traditional convolutional networks. The output head will consist of an 8×8×73 tensor, representing all possible moves in chess.***\n",
    "\n",
    "***To train this network, we will set up a reinforcement learning environment using OpenAI Gym. We will create self-play agents that use the probabilities generated by the Policy NN, combined with Monte Carlo Tree Search (MCTS), to play games against themselves. At each step of the game, the agent will record the current board state, the move distribution obtained from MCTS, and the final win/loss outcome. The MCTS component ensures that move selection is biased for less explored branches, striking a balance between exploration and exploitation through mechanisms such as the PUCT (Predictor + Upper Confidence Bound for Trees) formula.***\n",
    "\n",
    "***After each self-play game, we will compute the loss function, which consists of three key terms:***\n",
    "\n",
    "***Mean Squared Error (MSE) loss for win percentage prediction, ensuring that the value function correctly estimates the probability of winning from a given board state.***\n",
    "***Cross-Entropy Loss between the MCTS move distribution and the policy network’s move distribution, refining the policy network to match the improved move selection of MCTS.***\n",
    "***A regularization term to prevent overfitting and stabilize training.***\n",
    "***The loss will be backpropagated through gradient descent, updating the Policy NN weights. This process will repeat continuously, with the updated policy network generating improved move distributions, leading to stronger self-play agents over time.***\n",
    "\n",
    "***This iterative cycle of self-play, MCTS-guided move selection, and policy refinement ensures that the model improves progressively, discovering stronger moves while maintaining a balance between exploration (searching for new strategies) and exploitation (using known strong moves effectively).***\n",
    "\n",
    "- In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.  \n",
    "\n",
    "- If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "- If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metric\n",
    "***Elo Rating System (Primary Metric)***\n",
    "\n",
    "- ***Mathematical Formulation:***\n",
    "    - ***Final ELO Calculation: R' = R + K(S - E)***\n",
    "        - *R': New rating*\n",
    "        - *R: Old rating*\n",
    "        - *K: Constant to dictate how much to change the rating (analogous to the learning rate ${\\alpha}$ in MC, DP, and TD)*\n",
    "        - *S: Actual score (1 for win, 0.5 for draw, or 0 for loss)*\n",
    "        - *E: Expected score*\n",
    "    - ***Expected Score Calculation:*** $E = \\frac{1}{1 + {10}^{\\frac{R_{opponent} - R}{400}}}$\n",
    "        - *$R_{opponent}$*: *Opponent's rating*\n",
    "        - *$R$: Player's rating*\n",
    "\n",
    "- ***Explanation***\n",
    "    - *The first equation represents the rule used to update the ELO rating of the model/player/bot after playing a game*\n",
    "        - *This equation will penalize the model significantly more if it loses to a player/bot with a lower ELO rating than if it was to lose to a player/bot with a higher ELO rating*\n",
    "        - *Subsequently, if the model beats a player/bot with a higher ELO rating, the new ELO rating of the bot will increase significantly more than if it were to beat a player/bot with a lower ELO rating*\n",
    "    - *The second equation represents the probability that the model will win the game based on their current ELO rating compared to their opponents ELO rating*\n",
    "        - *This equation helps quantify the capability of the model with respect to its opponent*\n",
    "            - *For instance, if the opponent and the model have a similar ELO rating, then $E_{opponent}$ and $E$ will be 0.5 since $R_{opponent}$ = $R$*\n",
    "            - *Subsequently, if one player is significantly better than the other, then their expected score will be closer to 1 and if a player is significantly worse than the other, then their expected score will be closer to 0*\n",
    "    - *This probability value will help proportionally adjust the ELO ratings based on the outcome with respect to the difference in ELO (from the start of the game)*\n",
    "        - *i.e: penalize the model significantly more if it loses to a player/bot with a lower ELO rating*\n",
    "    - *In general, the higher the ELO rating, the better the model/bot/player is*\n",
    "- ***Relevance***\n",
    "    - *The ELO score represents how good of a chess player the model is*\n",
    "        - *Important to differentiate that just because the model is a good chess player doesn't necessarily translate to the model always picking the best possible move at each state*\n",
    "    - *ELO updates are done with respect to the model's current skill level and the opponent's skill level*\n",
    "        - *Proportionally rewards/penalizes model (i.e: penalize the model significantly more if it loses to a player/bot with a lower ELO rating)*\n",
    "        - *The final ELO score of our model, for our project purposes, is derived through repeated matches with known chess engines of various ratings (will be elaborated on more in further sections)*\n",
    "    - The ELO rating of our model can be directly compared against that of other models to evaluate how well our model performs\n",
    "        - i.e: The ELO rating is a normalized metric, allowing direct comparison across different models and real players\n",
    "\n",
    "\n",
    "***Head-to-Head Matches against Popular Chess Engines***\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Probably you should include a learning curve to demonstrate how much better the model gets as you increase the number of trials\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Generally reinforement learning tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible.  Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?  Or you compare a completely different approach/alogirhtm to the problem? Whatever, this stuff is just serving suggestions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "\n",
    "### Future work\n",
    "Looking at the limitations and/or the toughest parts of the problem and/or the situations where the algorithm(s) did the worst... is there something you'd like to try to make these better.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "* **Informed Consent**\n",
    "  * We will make sure to inform the owner of the gym-chess environment and the owner of the chess play data about the use of their code and gain persmission to use their source code.\n",
    "* **Data Security**\n",
    "  * Since the owner of the gym-chess environment and chess play data may not want public display of their source code in a third-party's repository, we will make sure to keep the data secure and make our repository private, only granting access to appropriate individuals.\n",
    "* **Data Storage**\n",
    "  * We plan to delete any data that we have collected for this project after the conclusion of the project.\n",
    "* **Data Representation**\n",
    "  * While cleaning and ensuring the data can be used for our project, we will make sure not to implement any new policies, inputs, or bias to represent the data in the best way possible.\n",
    "* **Fair Play**\n",
    "  * We will not produce the chess bot with the intent to bypass any anti-cheating mechanisms and will work to the best of our abilities to ensure the chess bot is not used for any misuse.\n",
    "* **AI Bias**\n",
    "  * We will train the chess bot to have a dynamic style of play and ensure that it doesn't develop a preference for a certain style of play.\n",
    "* **Interpretability**\n",
    "  * We will make sure to use appropriate techniques and visualizations, if applicable, to explain our model's decision making process to the best of our ability.\n",
    "* **Auditability**\n",
    "  * We will make sure that all code, data, visualizations, and results produced through our project will be reasonable and reproducible.\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"Thomas McGrath, Andrei Kapishnikov, Nenad Tomašev, Adam Pearce, Martin Wattenberg, Demis Hassabis, Been Kim, Ulrich Paquet, and Vladimir Kramnik\"></a>1.[^](#McGrath): McGrath, Thomas et. al., (2022) Acquisition of chess knowledge in AlphaZero. *Proceedings of the National Academy of Sciences (PNAS)*. https://www.pnas.org/doi/epub/10.1073/pnas.2206625119<br> \n",
    "<a name=\"Hitanshu Panchal, Siddhant Mishra, and Varsha Shrivastava\"></a>2.[^](#lorenz): Panchal, Hitanshu et. al., (2021) Chess Moves Prediction using Deep Learning Neural Networks. *International Conference on Advances in Computing and Communications (ICACC)*. https://ieeexplore.ieee.org/abstract/document/9708405<br> \n",
    "\n",
    "\n",
    "<a name=\"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabish\\\"></a>3.[^](#Silver) Silver, Hubert, et. al., (2017) Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm. *DeepMind*. https://arxiv.org/pdf/1712.01815 <br>\n",
    "\n",
    "<a name =\"Daniel Monroe, Philip A. Chalmers\"></a>4.[^](#Monroe) Monroe, Chalmers (2024) Mastering Chess with a Transformer Model. https://arxiv.org/pdf/2409.12272"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
